{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2746b07-85b5-456b-880f-4afc6c45c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    MATHEMATICAL LOGIC:\n",
    "    Since Transformers process sequences in parallel, they lack an inherent sense of time.\n",
    "    We inject a sinusoidal signal PE(pos, 2i) = sin(pos / 10000^(2i/d_model)) to encode \n",
    "    temporal order. This allows the model to distinguish between an acceleration spike \n",
    "    at t=65 versus steady-state motion at t=80.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # div_term defines the wavelength of the sinusoidal signals.\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Math: z_t = embedding_vector_t + PE_vector_t\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class PhysicsTransformerEstimator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=2,         # Subscript x_t: [vel, acc]\n",
    "        d_model=64,          # Latent dimension (d)\n",
    "        nhead=4,\n",
    "        # =========================================================================================\n",
    "        # MULTI-HEAD ATTENTION (nhead=4)\n",
    "        # =========================================================================================\n",
    "        # 1. SPLITTING: The d_model (64) is split into 4 heads of 16 dimensions each.\n",
    "        #    Math: head_dim = d_model // nhead = 16.\n",
    "        #\n",
    "        # 2. PARALLEL PHYSICS: Each head has its own W_Q, W_K, and W_V matrices.\n",
    "        #    This allows Head 1 to spotlight the 'Spike' while Head 2 spotlights the 'Slide'.\n",
    "        #\n",
    "        # 3. CONCATENATION: The 4 outputs [Batch, 20, 16] are glued together to form [Batch, 20, 64].\n",
    "        #    Final Context = Concat(head_1, head_2, head_3, head_4) @ W_O.\n",
    "        #\n",
    "        # 4. WHY 4?: It provides enough diversity to capture mass (transient) and \n",
    "        #    friction (steady-state) without making the feature space too small.\n",
    "        # =========================================================================================\n",
    "        num_encoder_layers=2, \n",
    "        dim_feedforward=128, \n",
    "        seq_len=20,          # T = 20\n",
    "        dropout=0.1          \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # ============================================================\n",
    "        # STEP 1: CONTINUOUS EMBEDDING & ENCODER\n",
    "        # Math: z_t = W_in * x_t + b_in\n",
    "        # W_in and b_in are TRAINABLE parameters. They learn how to \n",
    "        # map raw kinematics into a high-dimensional physical feature space.\n",
    "        # ============================================================\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len=seq_len + 10)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            batch_first=True, dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        # ============================================================\n",
    "        # STEP 2: DECODER (GENERATING h_t_dec)\n",
    "        # Math: h_t_dec = CrossAttention(q_t_query, H_enc, H_enc)\n",
    "        # h_t_dec is the hidden state for time step 't' that integrates \n",
    "        # the entire motion context into a force-ready representation.\n",
    "        # ============================================================\n",
    "        \n",
    "        # Parameter: Learned Time Queries\n",
    "        # Action: A trainable matrix representing \"what the model wants to know\" about each time step.\n",
    "        # MATH: In an LLM, Query = W_Q * Embedding. In your case, these ARE the Queries (Parameters).\n",
    "        # Instead of calculating them from the input, the model optimizes these 64D vectors \n",
    "        # during training to become \"templates\" for each of the 20 time steps.\n",
    "        self.time_queries = nn.Parameter(torch.randn(1, seq_len, d_model))\n",
    "        \n",
    "        # Layer: Cross-Attention\n",
    "        # Logic: The model performs the standard dot-product attention: \n",
    "        # Score = (q_dec * W_Q) @ (h_enc * W_K).T\n",
    "        # Even though q_dec starts as a fixed parameter, the 'MultiheadAttention' layer \n",
    "        # still applies a learned W_Q weight matrix to it during the forward pass.\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, nhead, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.norm_dec = nn.LayerNorm(d_model)\n",
    "        self.ffn_dec = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        self.norm_ffn = nn.LayerNorm(d_model)\n",
    "\n",
    "        # ============================================================\n",
    "        # STEP 3: INTERMEDIATE FORCE SEQUENCES\n",
    "        # Math: F_net_t = MLP_net(h_t_dec) and F_fric_t = MLP_fric(h_t_dec)\n",
    "        # These sequences decouple the transient (ma) from the steady (mu*m*g).\n",
    "        # ============================================================\n",
    "        self.net_force_mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model), nn.ReLU(), nn.Linear(d_model, d_model)\n",
    "        ) \n",
    "        self.fric_force_mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model), nn.ReLU(), nn.Linear(d_model, d_model)\n",
    "        ) \n",
    "\n",
    "        # ============================================================\n",
    "        # STEP 4: GLOBAL READOUTS (MASS & MU)\n",
    "        # We use Cross-Attention as a \"Spotlight\" to reduce sequences to scalars.\n",
    "        # Alpha_t^m: Weight assigned to frame t for mass calculation.\n",
    "        # ============================================================\n",
    "        \n",
    "        # --- A. MASS ESTIMATION ---\n",
    "        # Logic: q_mass learns to spotlight the acceleration peak (e.g., t=65).\n",
    "        \n",
    "        # Parameter: Mass Query (\"Spotlight\")\n",
    "        # MATH: Unlike an LLM where queries change per-word, self.q_mass is a GLOBAL Query.\n",
    "        # Think of it as a specialized \"sensor\" that is permanently tuned to the 64D \n",
    "        # frequency of a mass-impact event. It is optimized through backpropagation \n",
    "        # to have a high dot-product similarity with the 'feat_net' vectors at t=65.\n",
    "        self.q_mass = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.mass_attn = nn.MultiheadAttention(d_model, 1, batch_first=True)\n",
    "        self.mass_pred_mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, 64), nn.ReLU(), nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "        # --- B. FRICTION ESTIMATION ---\n",
    "        # Physics: F_fric = mu * m * g. \n",
    "        # Logic: We pass predicted Mass into the MLP so it can solve for mu = F / (m*g).\n",
    "\n",
    "        # Parameter: Friction Query\n",
    "        # MATH: Similarly, q_fric is a global learned parameter. It doesn't rely on \n",
    "        # input multiplication to exist; it is a dedicated query vector that has \n",
    "        # learned the \"look\" of steady-state latent friction features (t=75-83).\n",
    "        self.q_fric = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.fric_attn = nn.MultiheadAttention(d_model, 1, batch_first=True)\n",
    "        self.mu_pred_mlp = nn.Sequential(\n",
    "            nn.Linear(d_model + 1, 64), nn.ReLU(), nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, extracted_acc, extracted_vel):\n",
    "        # x_t shape: [Batch, 20, 2]\n",
    "        x = torch.cat([extracted_vel, extracted_acc], dim=-1) \n",
    "        B, T, _ = x.shape\n",
    "\n",
    "        # ENCODER: Motion Context (h_enc)\n",
    "        # Every frame looks at every other frame to identify \"events\" (spikes/sliding).\n",
    "        # EXAMPLE: At t=65 (the deceleration spike), self-attention \"notices\" the sudden drop \n",
    "        # in velocity compared to t=50. It weights these frames highly to identify a \n",
    "        # \"Transient Event,\" which is the critical window for mass estimation.\n",
    "        # Conversely, at t=80, the model identifies \"Steady Sliding\" because acceleration \n",
    "        # remains near zero while velocity is constant.\n",
    "        \n",
    "        # Action: Project the 2D input [v, a] into a 64D Physical Feature Space.\n",
    "        # Once trained, W_in acts as a physical feature extractor. \n",
    "        # EXAMPLE: Just as \"King\" and \"Man\" are close in LLM embeddings, W_in maps \n",
    "        # (v=0.04, a=-3.5) to a \"Heavy Brake\" vector and (v=0.04, a=0.0) to a \n",
    "        # \"Steady Slide\" vector. These distinct regions in 64D space allow the \n",
    "        # Transformer to decouple inertial effects from surface friction.\n",
    "        z = self.input_proj(x)\n",
    "        z = self.pos_encoder(z)\n",
    "        h_enc = self.transformer_encoder(z)\n",
    "\n",
    "\n",
    "        # =========================================================================================\n",
    "        # 1. DECODER CROSS-ATTENTION: attn_output, _ = self.cross_attn(query=q_dec, key=h_enc, value=h_enc)\n",
    "        # =========================================================================================\n",
    "        # Even though we pass 'q_dec' and 'h_enc' directly, nn.MultiheadAttention \n",
    "        # INTERNALLY contains weight matrices W_Q, W_K, and W_V.\n",
    "        #\n",
    "        # Process per Attention Head (Head_i):\n",
    "        # A. Projection: \n",
    "        #    - Query (Q_i) = q_dec * W_i_Q (Your 20 learned time templates)\n",
    "        #    - Key (K_i) = h_enc * W_i_K (The 20 encoder context frames)\n",
    "        #    - Value (V_i) = h_enc * W_i_V (The actual features/information at those frames)\n",
    "        #\n",
    "        # B. Calculation:\n",
    "        #    - Score_Matrix = (Q_i @ K_i.T) / sqrt(d)\n",
    "        #    - This matrix is [20 queries x 20 keys], representing how much each query looks at each key.\n",
    "        #\n",
    "        # C. Output Generation:\n",
    "        #    - Head_Output = Softmax(Score_Matrix) @ V_i\n",
    "        #\n",
    "        # =========================================================================================\n",
    "        # 2. SOFTMAX LOGIC: ONE QUERY vs. ALL KEYS\n",
    "        # =========================================================================================\n",
    "        # Softmax is applied ROW-WISE (one query vs. all keys).\n",
    "        #\n",
    "        # Math: For a specific Query 't', AttentionWeight(t, j) = exp(Score_t_j) / sum_across_all_j(exp(Score_t_j)).\n",
    "        #\n",
    "        # Purpose:\n",
    "        # - It normalizes the dot-product multiplication (which can be any real number) into a \n",
    "        #   probability distribution.\n",
    "        # - For 'q_mass', this ensures the \"spotlight\" focuses on the acceleration spike at t=65 \n",
    "        #   by giving it a probability near 1.0, while quiet frames get near 0.0.\n",
    "        #\n",
    "        # =========================================================================================\n",
    "        # 3. WHY NOT JUST SOFTMAX(QK.T)?\n",
    "        # =========================================================================================\n",
    "        # We use Softmax( (QK.T) / sqrt(d) ) to stabilize gradients.\n",
    "        #\n",
    "        # Reason:\n",
    "        # - As the dimensionality 'd' increases, the magnitude of the dot product (Q @ K.T) grows large.\n",
    "        # - Large values push the Softmax function into regions where the gradient is extremely small.\n",
    "        # - Dividing by the scaling factor sqrt(d) keeps the values in a range where the model \n",
    "        #   can still learn effectively during training.\n",
    "        #\n",
    "        # =========================================================================================\n",
    "        # 4. VALUE MATRIX IN YOUR CASE: WHAT IS ADDED?\n",
    "        # =========================================================================================\n",
    "        # In the LLM \"fluffy creature\" example, 'fluffy' adds a \"texture\" signal to 'creature'.\n",
    "        #\n",
    "        # In your robotics case (Motion Context -> Force Sequence):\n",
    "        # - Query (q_dec_t): \"I am time slot t. What is the physical state here?\"\n",
    "        # - Key (h_enc_j): \"I am frame j, and I have a huge acceleration spike.\"\n",
    "        # - Value (v_j): Multiplication (h_enc_j * W_V).\n",
    "        #\n",
    "        # Conceptual Meaning of Value Multiplication:\n",
    "        # - If a frame is relevant (High Attention Score), the Value represents: \n",
    "        #   \"What specific force features should be written into this time slot?\"\n",
    "        # - For Sample 441, the Value at t=65 adds an \"11 Newton\" signal to the decoder state.\n",
    "        # - For Sample 2276, the Value at t=65 adds a \"9 Newton\" signal to the decoder state.\n",
    "        # - The model doesn't just pass the input; the Value matrix learns how to transform \n",
    "        #   kinematic history into force-generating features.\n",
    "        # =========================================================================================\n",
    "        q_dec = self.time_queries.expand(B, -1, -1)\n",
    "\n",
    "        # The layer internally applies W_Q to q_dec and W_K to h_enc here.\n",
    "        # attn_output, _ = self.cross_attn(query=q_dec, key=h_enc, value=h_enc)\n",
    "        attn_output, attn_weights = self.cross_attn(\n",
    "            query=q_dec, \n",
    "            key=h_enc, \n",
    "            value=h_enc, \n",
    "            average_attn_weights=False  # To visualize the individual attention patterns of the 4 heads within the cross_attn layer\n",
    "        )\n",
    "        h_dec = self.norm_dec(q_dec + attn_output)\n",
    "        h_dec = self.norm_ffn(h_dec + self.ffn_dec(h_dec))\n",
    "\n",
    "        # FORCE SEQUENCES: Decoupling Dynamics\n",
    "        # feat_net_t targets the Net Force Signal.\n",
    "        # feat_fric_t targets the Friction Signal.\n",
    "        feat_net = self.net_force_mlp(h_dec)\n",
    "        feat_fric = self.fric_force_mlp(h_dec)\n",
    "\n",
    "        # GLOBAL PROPERTY READOUT: Mass Prediction\n",
    "        # -------------------------------------------------------------------------\n",
    "        # MATH: Inside mass_attn, W_Q is applied to q_m_batch and W_K to feat_net.\n",
    "        # Query: \"Looking for inertial spikes.\"\n",
    "        # Key:   \"Answering with latent net-force signatures (e.g., the 11N peak).\"\n",
    "        # -------------------------------------------------------------------------\n",
    "        q_m_batch = self.q_mass.expand(B, -1, -1)\n",
    "        mass_ctx, _ = self.mass_attn(query=q_m_batch, key=feat_net, value=feat_net)\n",
    "        mass_pred = self.mass_pred_mlp(mass_ctx.squeeze(1))\n",
    "\n",
    "        # GLOBAL PROPERTY READOUT: Friction Prediction\n",
    "        # -------------------------------------------------------------------------\n",
    "        # MATH: Inside fric_attn, W_Q is applied to q_f_batch and W_K to feat_fric.\n",
    "        # Query: \"Looking for steady-state sliding.\"\n",
    "        # Key:   \"Answering with latent friction-force signatures.\"\n",
    "        # -------------------------------------------------------------------------\n",
    "        q_f_batch = self.q_fric.expand(B, -1, -1)\n",
    "        fric_ctx, _ = self.fric_attn(query=q_f_batch, key=feat_fric, value=feat_fric)\n",
    "        \n",
    "        # Concatenate Mass for Physics Consistency\n",
    "        # Math: mu = f(Mass_Context, Mass_Scalar)\n",
    "        fric_input = torch.cat([fric_ctx.squeeze(1), mass_pred], dim=-1)\n",
    "        mu_pred = self.mu_pred_mlp(fric_input)\n",
    "\n",
    "        # Result: [Batch, 2] -> [Mass, Mu]\n",
    "        return torch.cat([mass_pred, mu_pred], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7d1e82-667a-4fba-b5ac-4a68db093e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# LOAD CHECKPOINT & VISUALIZE\n",
    "# ==========================================\n",
    "\n",
    "# --- CONFIGURATION: WHICH MODEL TO LOAD? ---\n",
    "# Set this to the epoch number you want to load (e.g., 50000)\n",
    "# Or set to None to use the model currently in memory (if you just finished training)\n",
    "LOAD_EPOCH = 100\n",
    "CRITERION_TYPE = \"mse\"\n",
    "LOSS_TYPE = \"data\"\n",
    "CHECKPOINT_DIR = f\"checkpoints/{CRITERION_TYPE}_{LOSS_TYPE}\"\n",
    "\n",
    "# 1. Initialize Model Structure (Must match training config)\n",
    "#    (We re-initialize to ensure we are testing a clean state, or if running this later)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PhysicsTransformerEstimator(\n",
    "    input_dim=2, \n",
    "    d_model=64, \n",
    "    nhead=4, \n",
    "    num_encoder_layers=2, \n",
    "    seq_len=20 # Make sure this matches your data prep\n",
    ").to(device)\n",
    "\n",
    "# 2. Load Weights\n",
    "if LOAD_EPOCH is not None:\n",
    "    load_path = f\"{CHECKPOINT_DIR}/transformer_epoch{LOAD_EPOCH}.pth\"\n",
    "    \n",
    "    if os.path.exists(load_path):\n",
    "        print(f\"üîÑ Loading model from: {load_path}\")\n",
    "        state_dict = torch.load(load_path, map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        print(\"‚úÖ Model weights loaded successfully!\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: Checkpoint not found at {load_path}\")\n",
    "        print(\"   Using random initialization or current model state.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Using current model state (no file loaded).\")\n",
    "\n",
    "# 3. VISUALIZATION\n",
    "print(\"\\n--- Running Evaluation ---\")\n",
    "\n",
    "# A. Loss Curve (Only available if you just trained, otherwise skip)\n",
    "if 'train_losses' in locals() and len(train_losses) > 0:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "    plt.plot(val_losses, label='Validation Loss', linewidth=2)\n",
    "    plt.title(f'Training Curve (Loaded Epoch: {LOAD_EPOCH})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No training loss history found in memory (skipped plot).\")\n",
    "\n",
    "# B. Prediction Check (First 5 samples of Val Set)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a batch from validation loader\n",
    "    try:\n",
    "        sample_acc, sample_vel, sample_y = next(iter(val_loader))\n",
    "    except NameError:\n",
    "        print(\"‚ùå Error: 'val_loader' is not defined. Please run Data Preparation first.\")\n",
    "        sample_acc = None\n",
    "\n",
    "    if sample_acc is not None:\n",
    "        sample_acc, sample_vel = sample_acc.to(device), sample_vel.to(device)\n",
    "        preds = model(sample_acc, sample_vel).cpu()\n",
    "        \n",
    "        print(\"\\n--- Sample Predictions (Val Set) ---\")\n",
    "        print(f\"{'GT Mass':<10} {'Pred Mass':<10} | {'GT Mu':<10} {'Pred Mu':<10}\")\n",
    "        print(\"-\" * 45)\n",
    "        for i in range(5):\n",
    "            print(f\"{sample_y[i,0]:.4f}     {preds[i,0]:.4f}     | {sample_y[i,1]:.4f}     {preds[i,1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
